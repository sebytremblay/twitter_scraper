{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Global Constants"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T05:56:49.356192Z","iopub.status.busy":"2024-05-17T05:56:49.355752Z","iopub.status.idle":"2024-05-17T05:56:49.365142Z","shell.execute_reply":"2024-05-17T05:56:49.364299Z","shell.execute_reply.started":"2024-05-17T05:56:49.356158Z"},"trusted":true},"outputs":[],"source":["# Kaggle data paths\n","run_on_kaggle = False\n","if run_on_kaggle:\n","    input_base_path = \"/kaggle/input/financial-tweets-stock-identifier/gpu_version\"\n","    output_base_path = \"/kaggle/working\"\n","else:\n","    input_base_path = \".\"\n","    output_base_path = \".\"\n","\n","# Establish column names\n","tweet_col = 'Tweet'\n","ticker_col = 'Regex Ticker'\n","\n","# Set the max length of the tweet\n","max_len = 256\n","\n","# Input and output data paths\n","raw_data_path = f\"{input_base_path}/data/ticker_tweets.csv\"\n","processed_data_path = raw_data_path.replace(\".csv\", \"_processed.csv\")\n","output_dir = f\"{output_base_path}/bert_models\"\n","tokenizer_output_dir = f'{output_dir}/tuned_tokenizer'\n","model_output_dir = f'{output_dir}/tuned_tokenizer'\n","label_encoder_output_dir = f'{output_dir}/label_encoder.pkl'\n","\n","# Establish the size of the dataset. Set to -1 to use the entire dataset.\n","data_size = 50\n","\n","# Determines whether the data or models should be forcefully reloaded\n","force_data_reload = False\n","force_model_reload = False\n","\n","# Determine training epochs. Set to -1 to run indefinitely.\n","EPOCHS = 1\n","BATCH_SIZE = 32"]},{"cell_type":"markdown","metadata":{},"source":["## Kaggle Environment Setup"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","\n","if run_on_kaggle:\n","    # Install necessary dependencies\n","    !pip install -r {input_base_path}/requirements.txt\n","    \n","    # Custom base paths\n","    input_base_path = \"/kaggle/input/financial-tweets-stock-identifier/gpu_version\"\n","    output_base_path = \"/kaggle/working\"\n","    \n","    # Add utils module to the Python path\n","    sys.path.append('/kaggle/input/financial-tweets-stock-identifier/gpu_version')"]},{"cell_type":"markdown","metadata":{},"source":["## Data Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import importlib\n","from utils import data_cleaning as dc\n","importlib.reload(dc)\n","\n","df = dc.init_df(force_data_reload, raw_data_path, processed_data_path, data_size, tweet_col)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T05:56:53.899176Z","iopub.status.busy":"2024-05-17T05:56:53.898881Z","iopub.status.idle":"2024-05-17T05:56:53.908905Z","shell.execute_reply":"2024-05-17T05:56:53.907800Z","shell.execute_reply.started":"2024-05-17T05:56:53.899151Z"},"trusted":true},"outputs":[],"source":["import os\n","import pickle\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Create the output directory if it doesn't exist\n","os.chmod(output_dir, 0o755)\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Force reload the models if missing a required dependency\n","for path in [tokenizer_output_dir, model_output_dir, label_encoder_output_dir]:\n","    if not os.path.exists(path):\n","        force_model_reload = True\n","        \n","# Define the label encoder\n","if force_model_reload:\n","    label_encoder = LabelEncoder()\n","    df['label'] = label_encoder.fit_transform(df[ticker_col])\n","else:\n","    label_encoder = pickle.load(open(label_encoder_output_dir, 'rb'))\n","    new_labels = df[ticker_col].unique()\n","    label_encoder.fit(new_labels)\n","    df['label'] = label_encoder.transform(df[ticker_col])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T05:56:53.910621Z","iopub.status.busy":"2024-05-17T05:56:53.910286Z","iopub.status.idle":"2024-05-17T05:56:53.927953Z","shell.execute_reply":"2024-05-17T05:56:53.926868Z","shell.execute_reply.started":"2024-05-17T05:56:53.910590Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split the data into train and test sets\n","train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T05:56:53.929886Z","iopub.status.busy":"2024-05-17T05:56:53.929289Z","iopub.status.idle":"2024-05-17T05:57:12.284028Z","shell.execute_reply":"2024-05-17T05:57:12.283244Z","shell.execute_reply.started":"2024-05-17T05:56:53.929854Z"},"trusted":true},"outputs":[],"source":["from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","import torch\n","\n","# Define where to load the model from\n","base_model = 'roberta-base'\n","tokenizer_path = base_model if force_model_reload else tokenizer_output_dir\n","model_path = base_model if force_model_reload else model_output_dir\n","model_args = { \"num_labels\": len(df[ticker_col].unique()) } if force_model_reload else {}\n","\n","# Initialize RoBERTa\n","tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n","model = RobertaForSequenceClassification.from_pretrained(model_path, **model_args)\n","\n","# Adjust for changes in labels\n","num_labels = len(label_encoder.classes_)\n","if model.num_labels != num_labels:\n","    model.num_labels = num_labels\n","    model.classifier = torch.nn.Linear(in_features=model.classifier.in_features, out_features=num_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T05:57:12.285514Z","iopub.status.busy":"2024-05-17T05:57:12.285083Z","iopub.status.idle":"2024-05-17T05:57:12.302205Z","shell.execute_reply":"2024-05-17T05:57:12.301287Z","shell.execute_reply.started":"2024-05-17T05:57:12.285487Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from utils import tweet_identification as ti\n","importlib.reload(ti)\n","\n","# Create DataLoaders\n","train_dataset = ti.TweetDataset(\n","    tweets=train_df[tweet_col].to_numpy(),\n","    labels=train_df['label'].to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=max_len\n",")\n","val_dataset = ti.TweetDataset(\n","    tweets=val_df[tweet_col].to_numpy(),\n","    labels=val_df['label'].to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=max_len \n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Training & Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T05:57:12.303519Z","iopub.status.busy":"2024-05-17T05:57:12.303195Z","iopub.status.idle":"2024-05-17T05:57:12.610701Z","shell.execute_reply":"2024-05-17T05:57:12.609701Z","shell.execute_reply.started":"2024-05-17T05:57:12.303487Z"},"trusted":true},"outputs":[],"source":["from torch.optim import AdamW\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","# Define training parameters\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model = model.to(device)\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T05:57:12.630478Z","iopub.status.busy":"2024-05-17T05:57:12.630162Z","iopub.status.idle":"2024-05-17T05:58:41.884857Z","shell.execute_reply":"2024-05-17T05:58:41.883613Z","shell.execute_reply.started":"2024-05-17T05:57:12.630454Z"},"trusted":true},"outputs":[],"source":["import pickle\n","import time\n","importlib.reload(ti)\n","\n","# Only save model when surpassing highest training accuracy\n","max_val_acc = 0\n","\n","# Run training loop\n","epoch = 0\n","while epoch == -1 or epoch < EPOCHS:\n","    print(f'Epoch {epoch + 1}/{EPOCHS}')\n","    print('-' * 10)\n","    start_time = time.time()\n","\n","    # Train the model\n","    train_acc, train_loss = ti.train_epoch(\n","        model,\n","        train_loader,\n","        optimizer,\n","        device,\n","        scheduler=None\n","    )\n","    print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","    # Evaluate the model on the validation set\n","    val_acc, val_loss = ti.eval_model(\n","        model,\n","        val_loader,\n","        device\n","    )\n","    print(f'Val loss {val_loss} accuracy {val_acc}')\n","\n","    # Save the model\n","    if val_acc > max_val_acc:\n","        print(f'Surpassed previous highest accuracy ({max_val_acc}). Saving model')\n","        max_val_acc = val_acc\n","        \n","        model.save_pretrained(model_output_dir)\n","        tokenizer.save_pretrained(tokenizer_output_dir)\n","        with open(label_encoder_output_dir, 'wb') as f:\n","            pickle.dump(label_encoder, f)\n","    \n","    # Do not increase epoch when running indefinitely\n","    if epoch >= 0:\n","        epoch += 1\n","    print(f'Epoch time: {time.time() - start_time}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T05:58:41.885749Z","iopub.status.idle":"2024-05-17T05:58:41.886123Z","shell.execute_reply":"2024-05-17T05:58:41.885959Z","shell.execute_reply.started":"2024-05-17T05:58:41.885944Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import classification_report\n","import numpy as np\n","\n","# Evaluation\n","y_preds = []\n","y_true = []\n","\n","model.eval()\n","with torch.no_grad():\n","    for d in val_loader:\n","        input_ids = d['input_ids'].to(device)\n","        attention_mask = d['attention_mask'].to(device)\n","        labels = d['label'].to(device)\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        _, preds = torch.max(outputs.logits, dim=1)\n","\n","        y_preds.extend(preds.cpu().numpy())\n","        y_true.extend(labels.cpu().numpy())\n","\n","# Align the labels\n","labels = sorted(list(set(y_true)))\n","target_names = label_encoder.inverse_transform(np.arange(len(label_encoder.classes_)))\n","print(classification_report(y_true, y_preds, labels=labels, target_names=target_names))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5009916,"sourceId":8429372,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
