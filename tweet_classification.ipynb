{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import importlib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import data_cleaning as dc\n",
    "\n",
    "# Data paths\n",
    "raw_data_path = \"./data/tweets.csv\"\n",
    "processed_data_path = raw_data_path.replace(\".csv\", \"_processed.csv\")\n",
    "\n",
    "# Establish column names\n",
    "tweet_col = 'Tweet'\n",
    "ticker_col = 'Stock Name'\n",
    "\n",
    "# Establish the size of the dataset. Set to -1 to use the entire dataset.\n",
    "data_size = 500\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(raw_data_path)\n",
    "if data_size > 0:\n",
    "    df = df.sample(data_size)\n",
    "\n",
    "# Preprocess the tweet column\n",
    "df[tweet_col] = df[tweet_col].apply(lambda x: dc.preprocess_tweet(x))\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df[ticker_col])\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Output files for model\n",
    "tokenizer_output_dir = './bert_models/finetuned_bert_tokenizer'\n",
    "model_output_dir = './bert_models/finetuned_bert_model'\n",
    "\n",
    "# Initialize the tokenizer\n",
    "if not os.path.exists(tokenizer_output_dir):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokenizer.save_pretrained(tokenizer_output_dir)\n",
    "    print(f\"Initalized and saved tokenizer to {tokenizer_output_dir}\")\n",
    "else:\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_output_dir)\n",
    "    print(f\"Loaded tokenizer from {tokenizer_output_dir}\")\n",
    "\n",
    "# Initalize the model\n",
    "if not os.path.exists(model_output_dir):\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df[ticker_col].unique()))\n",
    "    model.save_pretrained(model_output_dir)\n",
    "    print(f\"Initalized and saved model to {model_output_dir}\")\n",
    "else:\n",
    "    model = BertForSequenceClassification.from_pretrained(model_output_dir)\n",
    "    print(f\"Loaded model from {model_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils import tweet_identification as ti\n",
    "importlib.reload(ti)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = ti.TweetDataset(\n",
    "    tweets=train_df[tweet_col].to_numpy(),\n",
    "    labels=train_df['label'].to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=64\n",
    ")\n",
    "val_dataset = ti.TweetDataset(\n",
    "    tweets=val_df[tweet_col].to_numpy(),\n",
    "    labels=val_df['label'].to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=64\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ti)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Train the model\n",
    "    train_acc, train_loss = ti.train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler=None\n",
    "    )\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_acc, val_loss = ti.eval_model(\n",
    "        model,\n",
    "        val_loader,\n",
    "        device\n",
    "    )\n",
    "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('finetuned_bert_model')\n",
    "tokenizer.save_pretrained('finetuned_bert_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation\n",
    "y_preds = []\n",
    "y_true = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for d in val_loader:\n",
    "        input_ids = d['input_ids'].to(device)\n",
    "        attention_mask = d['attention_mask'].to(device)\n",
    "        labels = d['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "        y_preds.extend(preds.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "# Align the labels\n",
    "labels = sorted(list(set(y_true)))\n",
    "target_names = label_encoder.inverse_transform(labels)\n",
    "\n",
    "print(classification_report(y_true, y_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dc)\n",
    "\n",
    "sample_tweet = \"The iPhone is a great product. I think Apple is a great company.\"\n",
    "\n",
    "# Preprocess the tweet\n",
    "sample_tweet = dc.preprocess_tweet(sample_tweet)\n",
    "\n",
    "# Tokenize the tweet\n",
    "inputs = tokenizer(sample_tweet, return_tensors='pt')\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get the predicted label\n",
    "_, preds = torch.max(outputs.logits, dim=1)\n",
    "print(label_encoder.inverse_transform(preds.cpu().numpy())[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
